{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fc5f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "156837b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        self._backward = lambda: None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(label={self.label}, data={self.data}, grad={self.grad})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad  # d(other)/d(loss) = d(other)/d(out) * d(out)/d(loss) = 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad  # += to be able to do a+a, Otherwise the gradients will overwrite each other. they should be added.\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):  # When it cant do mul, try the other way around. \n",
    "        return self * other     # input is other, self. Already swithced\n",
    "    \n",
    "    # Implementing tanh x = (e^2x - 1) / (e^2x + 1)\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tahn')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad  # local derivive (tanh) is: 1 - tanh(x)**2 = 1-t**2\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        \"\"\"other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data / other.data, (self, other), '/')  # value, prev, op (no label)\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * 1.0 / other.data\n",
    "            other.grad += out.grad * -1.0 * self.data / other.data**2\n",
    "        out._backward = _backward\"\"\"\n",
    "        return self * (other ** -1)\n",
    "\n",
    "    def __rdiv__(self, other):\n",
    "        return self / other\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        # other = other if isinstance(other, Value) else Value(other)\n",
    "        assert isinstance(other, (int, float)), \"Only power of int or float is supported.\"\n",
    "        out = Value(self.data ** other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * self.data ** (other -1) * out.grad\n",
    "            #other.grad += self.data ** other * math.log(self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "     \n",
    "    def backward(self):\n",
    "        # topological sort\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0  # set starting to be the last node, having gradient 1 (how much does last node change if you change itself?)\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f29cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.tanh(); o.label = 'o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18c620cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4999999999999999\n",
      "0.4999999999999999\n"
     ]
    }
   ],
   "source": [
    "o.backward()\n",
    "print(x1w1.grad)\n",
    "print(x2w2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a1f7243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # x * w + b\n",
    "        assert len(x)==len(self.w), \"input length does not match\"\n",
    "        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b) # second parameter in sum sets the start to sum from.\n",
    "        out = act.tanh()                                        # obs zip cuts the logest list if not equal\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "class Layer:\n",
    "    \n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self, x): \n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs)==1 else outs\n",
    "\n",
    "    def parameters(self):\n",
    "        #print([n.parameters() for n in self.neurons])  # this one keeps the nested lists\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, nin, nouts):  # Nouts being a list of all layers size\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        # [l.parameters() for l in self.layers]  # this one keeps the nested lists\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f55d191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLP(3, [4, 4, 1])\n",
    "xs = [\n",
    "    [1.0, 2.0, 3.0], \n",
    "    [2.0, 3.0, 3.0], \n",
    "    [1.0, 1.0, 0.0], \n",
    "    [1.0, 1.0, 1.0]\n",
    "]\n",
    "ys = [1.0, 1.0, -1.0, -1.0]  # small values predicts -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "548918c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.995237104644893\n",
      "2.174614860351153\n",
      "1.0783247466216819\n",
      "0.5911642257789561\n",
      "0.3648335699323164\n",
      "0.24817241568023835\n",
      "0.18156480411076092\n",
      "0.14012151270796994\n",
      "0.11251249062112018\n",
      "0.09310505847357048\n",
      "-------------\n",
      "Predictions:\n",
      "[0.9166775765494444, 0.8308331026970641, -0.9367875978554346, -0.7986015389846126]\n"
     ]
    }
   ],
   "source": [
    "nr_epochs = 100\n",
    "lr = 0.01#1e-3\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    # Forward pass\n",
    "    ypred = [nn(x) for x in xs]\n",
    "    loss = sum((yp-yn)**2 for yn, yp in zip(ys, ypred))\n",
    "\n",
    "    # Backward pass AND zero gradients\n",
    "    for p in nn.parameters():\n",
    "        p.grad = 0.0\n",
    "    loss.backward()\n",
    "\n",
    "    # Uppdate weights\n",
    "    \n",
    "    for p in nn.parameters():\n",
    "        p.data += p.grad * -lr\n",
    "    if epoch % 10 == 0:\n",
    "        print(loss.data)\n",
    "\n",
    "print('-------------')\n",
    "print('Predictions:')\n",
    "print([y.data for y in ypred])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75fdf6ac",
   "metadata": {},
   "source": [
    " *Most common neural net mistakes:* \n",
    " 1) you didn't try to overfit a single batch first. \n",
    " 2) you forgot to toggle train/eval mode for the net. \n",
    " 3) you forgot to .zero_grad() (in pytorch) before .backward(). \n",
    " 4) you passed softmaxed outputs to a loss that expects raw logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "913ec014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value(label=, data=0.30747227717092224, grad=0.0035449865604668465)\n",
      "Value(label=, data=-0.025424505627471106, grad=0.001155635968813692)\n",
      "Value(label=, data=-0.41283872547346234, grad=0.0007267872569475826)\n",
      "Value(label=, data=-1.0479397968959554, grad=0.005805603935577617)\n",
      "\n",
      "Value(label=, data=0.4764488535167226, grad=-0.05188865748219201)\n",
      "Value(label=, data=-0.3527680234272192, grad=0.03637976928107434)\n",
      "Value(label=, data=-0.5009204153721958, grad=0.05111771577987495)\n",
      "Value(label=, data=0.6624933808826854, grad=-0.1309237514164218)\n",
      "\n",
      "Value(label=, data=-0.21067607403296473, grad=-0.02075844061724675)\n",
      "Value(label=, data=-0.8442942844375141, grad=-0.05381450134491602)\n",
      "Value(label=, data=0.8740996830269342, grad=-0.06030335399954203)\n",
      "Value(label=, data=-0.6742363469928555, grad=0.006055522562564565)\n",
      "\n",
      "Value(label=, data=-0.678639563388376, grad=-0.030987361163647247)\n",
      "Value(label=, data=0.5306312174296226, grad=-0.014836785870856024)\n",
      "Value(label=, data=-0.557436589917409, grad=-0.009136694889894104)\n",
      "Value(label=, data=0.3252528586605727, grad=-0.043912253131838115)\n",
      "\n",
      "\n",
      "Value(label=, data=-1.0479397968959554, grad=0.005805603935577617)\n",
      "Value(label=, data=0.6624933808826854, grad=-0.1309237514164218)\n",
      "Value(label=, data=-0.6742363469928555, grad=0.006055522562564565)\n",
      "Value(label=, data=0.3252528586605727, grad=-0.043912253131838115)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, p in enumerate(nn.layers[0].parameters()):\n",
    "    if i % 4 == 0: # 3inputs + bias\n",
    "        print('')\n",
    "    print(p)\n",
    "print('')\n",
    "print('')\n",
    "nod = 3\n",
    "w1s = nn.layers[0].parameters()\n",
    "print(w1s[nod])\n",
    "print(w1s[nod+4])\n",
    "print(w1s[nod+8])\n",
    "print(w1s[nod+12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac75a7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Layer at 0x10dec6440>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66153c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformerenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Jan 11 2023, 09:18:18) [Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2548c82742612426ce1057d1d8e79feaf56c699cf5590fd32694895dd30efb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
